{"cells":[{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.optimizers import Adam\n","import matplotlib.pyplot as plt\n","\n","# Load the MNIST dataset\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","# Preprocess the data\n","x_train = x_train.astype('float32') / 255.0  # Normalize pixel values to [0, 1]\n","x_test = x_test.astype('float32') / 255.0\n","y_train = to_categorical(y_train, 10)  # One-hot encode labels\n","y_test = to_categorical(y_test, 10)\n","\n","# Function to create the model with different activation functions\n","def create_model(activation_function):\n","    model = Sequential()\n","    model.add(Flatten(input_shape=(28, 28)))  # Flatten the 28x28 image into a 1D vector\n","    model.add(Dense(128, activation=activation_function))  # Hidden layer with 128 neurons\n","    model.add(Dense(10, activation='softmax'))  # Output layer with 10 neurons (for 10 classes)\n","    \n","    # Compile the model\n","    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n","    \n","    return model\n","\n","# Activation functions to experiment with\n","activation_functions = ['relu', 'sigmoid', 'tanh']\n","\n","# To store training history and results\n","history_dict = {}\n","\n","# Train the model with different activation functions\n","for activation_function in activation_functions:\n","    print(f\"\\nTraining model with {activation_function} activation function...\")\n","    \n","    model = create_model(activation_function)\n","    \n","    # Train the model\n","    history = model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test), verbose=2)\n","    \n","    # Store the history of training and validation accuracy\n","    history_dict[activation_function] = history.history\n","\n","    # Evaluate the model\n","    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n","    print(f\"Test accuracy with {activation_function}: {test_acc:.4f}\")\n","    \n","    # Plot accuracy and loss for each activation function\n","    plt.figure(figsize=(12, 6))\n","    \n","    plt.subplot(1, 2, 1)\n","    plt.plot(history.history['accuracy'], label='Train Accuracy')\n","    plt.plot(history.history['val_accuracy'], label='Test Accuracy')\n","    plt.title(f'{activation_function.capitalize()} - Accuracy')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","    \n","    plt.subplot(1, 2, 2)\n","    plt.plot(history.history['loss'], label='Train Loss')\n","    plt.plot(history.history['val_loss'], label='Test Loss')\n","    plt.title(f'{activation_function.capitalize()} - Loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    \n","    plt.show()\n","\n","# Summary of results\n","print(\"\\nSummary of results:\")\n","for activation_function in activation_functions:\n","    print(f\"{activation_function.capitalize()} - Final Test Accuracy: {history_dict[activation_function]['val_accuracy'][-1]:.4f}\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3f5b2fca-80ea-4f8b-807f-7debb845720a"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}